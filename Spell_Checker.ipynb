{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spell Checker",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrisSandy/TheSilenceOfNLP/blob/master/Spell_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2oP1uun77cIh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spell Checker\n",
        "\n",
        "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
        "\n",
        "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
        "\n",
        "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
        "\n",
        "    My siter|sister go|goes to Tonbury .\n",
        "    \n",
        "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
        "\n",
        "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
        "\n",
        "    My Mum goes out some_times|sometimes .\n",
        "    \n",
        "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
        "\n",
        "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
      ]
    },
    {
      "metadata": {
        "id": "TIVCSJV-7kDs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text Parser\n",
        "\n",
        "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
        "\n",
        "Then split your data into a test set of 100 lines and a training set."
      ]
    },
    {
      "metadata": {
        "id": "3zSM3Bt8NtV4",
        "colab_type": "code",
        "outputId": "760db8b8-e90e-4e25-e474-61668b3ed68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MgrndX3bHEBW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lines = open(r'/content/gdrive/My Drive/GYE06/CT5120_NLP/Data/holbrook.txt').readlines()\n",
        "data = []\n",
        "# Write your code here\n",
        "\n",
        "for line in lines:\n",
        "    words = line.split()\n",
        "    data_sent_dict = {\"original\": [], \"corrected\": [], \"indexes\": []}\n",
        "    for i, word in enumerate(words):\n",
        "        if word.find(\"|\") != -1:\n",
        "            word_epair = word.split(\"|\")\n",
        "            data_sent_dict[\"original\"].append(word_epair[0])\n",
        "            data_sent_dict[\"corrected\"].append(word_epair[1])\n",
        "            data_sent_dict[\"indexes\"].append(i)\n",
        "        else:\n",
        "            data_sent_dict[\"original\"].append(word)\n",
        "            data_sent_dict[\"corrected\"].append(word)\n",
        "    data.append(data_sent_dict)\n",
        "\n",
        "assert (data[2] == {\n",
        "    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'],\n",
        "    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'],\n",
        "    'indexes': [9]\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRSX4I0H7pSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
      ]
    },
    {
      "metadata": {
        "id": "Kt9aR2Gy7p1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  test = data[:100]\n",
        "  train = data[100:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hm5JL7cH7sLK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate Frequency: \n",
        "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
        "\n",
        "*Hint: use `Counter` to implement this so it may be called many times*"
      ]
    },
    {
      "metadata": {
        "id": "7ge0uHS-7uEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_dictionary = [w.lower() for s in train for w in s[\"corrected\"]]\n",
        "word_counts = Counter(word_dictionary)\n",
        "tot_words = sum([len(s[\"corrected\"]) for s in train])\n",
        "\n",
        "\n",
        "def unigram(word):\n",
        "    # Write your code here.\n",
        "    return word_counts[word.lower()]\n",
        "\n",
        "\n",
        "def prob(word):\n",
        "    return float(unigram(word)) / tot_words\n",
        "  \n",
        "# Test your code with the following\n",
        "assert(unigram(\"me\")==87)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w8r8QYj78GPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate Distance: \n",
        "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
      ]
    },
    {
      "metadata": {
        "id": "SV9Mu8P38IQE",
        "colab_type": "code",
        "outputId": "1d82ae70-6407-4496-cf54-997b94c6ba7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hm46Lbiz8K8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
        "\n",
        "1. Collect the set of all unique tokens in `train`\n",
        "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
        "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
        "\n",
        "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
      ]
    },
    {
      "metadata": {
        "id": "HoilAmFW8PCb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_candidates(token):\n",
        "    # Write your code here.\n",
        "\n",
        "    token_distances = {k: edit_distance(token.lower(), k) for k in word_dictionary}\n",
        "    candidates = [k for k, v in token_distances.items() if v == min(token_distances.values())]\n",
        "\n",
        "    return candidates\n",
        "        \n",
        "# Test your code as follows\n",
        "assert get_candidates(\"minde\") == ['mind', 'mine']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGY-eCkN8TIM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Correct Sentence:\n",
        "\n",
        "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
        "\n",
        "*Your solution to this should involve `get_candidates`*\n"
      ]
    },
    {
      "metadata": {
        "id": "dIGKE4_P8WGP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def correct(sentence):\n",
        "    # Write your code here\n",
        "\n",
        "    corrected_sentence = []\n",
        "\n",
        "    for word in sentence:\n",
        "        if word.lower() not in word_dictionary:\n",
        "            candidates = get_candidates(word)\n",
        "            if len(candidates) == 1:\n",
        "                corrected_sentence.append(candidates[0])\n",
        "            else:\n",
        "                candidates_prob = {c: prob(c) for c in candidates}\n",
        "                corrected_sentence.append(max(candidates_prob, key=candidates_prob.get))\n",
        "        else:\n",
        "            corrected_sentence.append(word)\n",
        "\n",
        "    return corrected_sentence\n",
        "\n",
        "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oG7jC6au8kka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Accuracy: \n",
        "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
      ]
    },
    {
      "metadata": {
        "id": "HSXTQypR8mdR",
        "colab_type": "code",
        "outputId": "4e734c58-f857-47e4-ed62-5bc69e1ce24c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "def accuracy(test):\n",
        "    # Write your code here\n",
        "\n",
        "    n = 0.0\n",
        "    n_correct = 0.0\n",
        "    for sentence in test:\n",
        "        n += len(sentence['corrected'])\n",
        "        sentence_corrected = correct(sentence['original'])\n",
        "        for (word_a, word_c) in zip(sentence['corrected'], sentence_corrected):\n",
        "            if word_a == word_c:\n",
        "                n_correct += 1\n",
        "\n",
        "    print(\"{} words matched out of {} words in corrected sentences\".format(n_correct, n))\n",
        "\n",
        "    return n_correct/n\n",
        "\n",
        "print(\"Accuracy of the current Spell Checker : {}\".format(accuracy(test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "946.0 words matched out of 1129.0 words in corrected sentences\n",
            "Accuracy of the current Spell Checker : 0.837909654562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nZK0jFSzXbe-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above spell checker algorithm has an accuracy of 83.7% which uses minimum edit distances and unigram probabilities for correcting the words."
      ]
    },
    {
      "metadata": {
        "id": "9b-r2JzD8_Zh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modifications\n",
        "\n",
        "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
        "\n",
        "* You may resources beyond those provided here.\n",
        "* You must **not use the test data** in this task.\n",
        "* Provide a short text describing what you intend to do and why. \n",
        "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
        "* Your implementation should not consist of more than 50 lines of code\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lecutures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
      ]
    },
    {
      "metadata": {
        "id": "vouAcJWoRp2J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Improvements to current Spell Checker\n",
        "\n",
        "1.   Ignore numbers from the correction algorithm\n",
        "2. inflection \n",
        "3. lemma\n",
        "4. Named entity reognision (ignore NNPs)\n",
        "5. Tenses\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3qVc_oJREaWC",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "32925a66-aae9-4bac-a2ca-10ef755ebb84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4220
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection u'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "CZVj06Wwgt71",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1. Named Entity recognision\n",
        "One of the problems with the above spell checker is that it is trying to correct proper nouns and named entities. For example word 'NIGEL' which is a proper noun is changed to 'night'.  This senario can be mitigated by identifying named entities/proper nouns. \n",
        "\n",
        "nltk's pos_tag function will identify proper nouns by tagging them with NNP. Hence tagging the entire sentence and ignoring any proper nouns (NNP tags) will avoid unnecessary correction.\n",
        "\n",
        "We can also use nltk's ne_chunk method to find named entity. ne_chunk gives a Tree structure with a lable defining the entity type. Function is_named_entity in the below code does the named entity check using ne_chunk method and returns True if a word is named Entity."
      ]
    },
    {
      "metadata": {
        "id": "ZTzRdW-UaBHw",
        "colab_type": "code",
        "outputId": "20c9dcd4-42e1-4389-fa3e-d575f5b461e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "def is_named_entity(word):\n",
        "    wordl = []\n",
        "    wordl.append(word)\n",
        "    entity = nltk.ne_chunk(nltk.pos_tag(wordl))\n",
        "    for chunk in entity:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            return True\n",
        "    return False\n",
        "  \n",
        "print(is_named_entity('NIGEL'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fmkJYi-9RzbP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2. Numbers identification\n",
        "\n",
        "Numeric entity can be ignored from the spell check. In order to do this, we can use nltk's pos_tags method which tags numbers with 'CD' tag. However pos_tag is not working for negative numbers, in which case they are identified as NN. Hence in below function, I have used both pos_tags method and also regular expressions to identify numeric entities. pos_tags also detects dates etc which can be ignored as well. "
      ]
    },
    {
      "metadata": {
        "id": "lphSSC9OlUWP",
        "colab_type": "code",
        "outputId": "6df627b3-96ca-44a1-e1e4-b939e30c505e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "numeric_re = re.compile(r\"[+-]?\\d+\")\n",
        "\n",
        "def is_numeric(word):\n",
        "    wordl = []\n",
        "    wordl.append(word)\n",
        "    tags = nltk.pos_tag(wordl)\n",
        "    \n",
        "    if re.match(numeric_re, word) is not None or tags[0][1] == 'CD':\n",
        "        return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "print(is_numeric(\"-48\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CqxwAwibmzuD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3. Use Wordnet to check correct words\n",
        "\n",
        "One of the problems with the spell checker is that it is correcting the correct words if it is not present in the training dictionary. Hence in order to avoid this, we can use nlkt wordnet corpus and compare each word with this corpus before passing it to the spell checker. The implementation is done in correct2 function below"
      ]
    },
    {
      "metadata": {
        "id": "DTQJ7z6Wru5n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 4. Lemmatizing\n",
        "\n",
        "Lemmatizing helps in bringing the same words with different tense in one form. For example saturdays is not identified by the spell checker because this form of the word is not present in the dictionary trained by training data. This may be the case with words present in the dictionary as well. Hence nltk's lemmarizer is used to convert these words into their actual form for easy matching with dictionary."
      ]
    },
    {
      "metadata": {
        "id": "aqA5Zut0tbcU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "word_dictionary_lemma = []\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for word in word_dictionary:\n",
        "  word_dictionary_lemma.append(lemmatizer.lemmatize(word))\n",
        "  word_dictionary_lemma.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_1SOvGgwjLK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 5. Linear Interpolation (Bigrams Model)\n",
        "\n",
        "After getting the candidate words by calculating minium distance of words, we are taking a simple unigram probability to choose from multiple words. This may result in using the most frequent word which is out of context in the sentence as replacement. To overcome this, we can use bigrams which calculates the probabilities by dividing matched bigrams pair by total number of bigrams. However as our training data is small, we might not get bigrams for most of the pairs which results in zero probability. In such cases we can use unigram probabilities. To combine both approaches, I have used Linear Interpolation method in which both bigram model and trigram model is used and weights are given to each model. These weights ($lambda1, lambda2$) can be adjusted to tune our model\n",
        "\n",
        "$P(word) = lambda1*P(unigram) + lambda2*P(bigram)$ where $ lambda1 + lambda2 = 1$\n",
        "\n",
        "This model can be extended to trigrams."
      ]
    },
    {
      "metadata": {
        "id": "P4E0YmfO0lN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.probability import ConditionalFreqDist\n",
        "\n",
        "all_words = [w.lower() for s in train for w in s[\"corrected\"]]\n",
        "bigrams_freqDist = ConditionalFreqDist(nltk.bigrams(all_words))\n",
        "\n",
        "\n",
        "def bigrams(word1, word2):\n",
        "    val = bigrams_freqDist[word1.lower()].get(word2.lower())\n",
        "    if val is not None:\n",
        "      return val\n",
        "    else:\n",
        "      return 0\n",
        "    \n",
        "\n",
        "def prob_bigrams(word1, word2):\n",
        "#   +1 is added to the denominator for smoothing\n",
        "    return float(bigrams(word1, word2)) / (sum(bigrams_freqDist[word1.lower()].values()) + 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0isXuqIfR98y",
        "colab_type": "code",
        "outputId": "007b86c5-ea43-42fe-91d8-7f9228fa7786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "  \n",
        "def correct2(sentence):\n",
        "\n",
        "    lambda1 = 0.5\n",
        "    lambda2 = 0.5\n",
        "    sentence_tags = nltk.pos_tag(sentence)\n",
        "    corrected_sentence = []\n",
        "    prev_word = ''\n",
        "    \n",
        "    for (word, tag) in sentence_tags:\n",
        "        if is_numeric(word) or is_named_entity(word) or tag == \"NNP\":\n",
        "            corrected_sentence.append(word)\n",
        "        elif word.lower() in wordnet.words() or \\\n",
        "             lemmatizer.lemmatize(word.lower()) in wordnet.words():\n",
        "            corrected_sentence.append(word)\n",
        "        elif word.lower() not in word_dictionary_lemma and \\\n",
        "             lemmatizer.lemmatize(word.lower()) not in word_dictionary_lemma:\n",
        "            candidates = get_candidates(word)\n",
        "            if (len(candidates) == 1):\n",
        "                corrected_sentence.append(candidates[0])\n",
        "            else:\n",
        "                candidates_uprob = {c: prob(c) for c in candidates}\n",
        "                candidates_bprob = {c:prob_bigrams(prev_word, c) for c in candidates}\n",
        "#               Linear Interpolation\n",
        "                candidates_prob = {k: lambda1 * candidates_uprob[k] + \n",
        "                               lambda2 * candidates_bprob[k] for k in candidates} \n",
        "                corrected_sentence.append(max(candidates_prob, key=candidates_prob.get))\n",
        "        else:\n",
        "            corrected_sentence.append(word)\n",
        "        prev_word = word\n",
        "        \n",
        "    return corrected_sentence\n",
        "\n",
        "print(correct2(['NIGEL', 'THRUSH', 'page', '48']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NIGEL', 'THRUSH', 'page', '48']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pCsb8sgH45-z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### More ideas of improvement\n",
        "\n",
        "\n",
        "\n",
        "1.   **Finding tense of the sentence:** Most of the errors after correcting each sentence by new model are because of tense. Hence by checking the grammer we can correct these errors. Tense can be identified by using the pos_tag and check the verbs for tense, but this is not an accurate method. We can either use nltk's Context Free Grammer (CFG) or language_check package which checks the grammer of the sentence and corrects it. \n",
        "2.   **Using Trigram model:** More symantics can be captured by using Trigram models, and using linear interpolation for all three with varying lambda values may give better accuracy\n",
        "3. Collecting more training data will expand out dictionary and improves the accuracy of the model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GLzaC6D28sK9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
      ]
    },
    {
      "metadata": {
        "id": "Hw6PzwWn7iEo",
        "colab_type": "code",
        "outputId": "6197ba8c-9f76-4bbb-ba44-2c54e2be29d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1667
        }
      },
      "cell_type": "code",
      "source": [
        "def accuracy(test):\n",
        "    # Write your code here\n",
        "\n",
        "    n = 0.0\n",
        "    n_correct = 0.0\n",
        "    for sentence in test:\n",
        "        n += len(sentence['corrected'])\n",
        "        sentence_corrected = correct2(sentence['original'])\n",
        "        for (word_a, word_c) in zip(sentence['corrected'], sentence_corrected):\n",
        "            if word_a == word_c:\n",
        "                n_correct += 1\n",
        "            else:\n",
        "               print(word_a + ' -- ' + word_c)\n",
        "\n",
        "    print(\"{} words matched out of {} words in corrected sentences\".format(n_correct, n))\n",
        "\n",
        "    return n_correct/n\n",
        "\n",
        "print(\"Accuracy of the current Spell Checker : {}\".format(accuracy(test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "goes -- go\n",
            "bellringing -- bell_ringing\n",
            "watch -- wash\n",
            "second -- sexton\n",
            "watch -- wash\n",
            "watch -- each\n",
            "cowboys -- cowboy\n",
            "club -- come\n",
            "watch -- each\n",
            "watch -- wash\n",
            "think -- thing\n",
            "TV -- tv\n",
            "square -- star\n",
            "eyes -- yes\n",
            "knocked -- nock\n",
            "at -- a\n",
            "killed -- kill\n",
            "saw -- see\n",
            "been -- bean\n",
            "knocked -- nock\n",
            "called -- cold\n",
            "came -- cam\n",
            "there -- the\n",
            "Her -- Here\n",
            "eyes -- yes\n",
            "have_to -- have\n",
            "else -- als\n",
            "wheel -- week\n",
            "wheel -- well\n",
            "sallies -- sally\n",
            "others -- other\n",
            "ring -- rings\n",
            "Cynthia's -- Cynthia\n",
            "gas -- gass\n",
            "masks -- mark\n",
            "floor -- door\n",
            "gas -- gass\n",
            "masks -- mark\n",
            "too -- to\n",
            "what -- was\n",
            "wash -- wish\n",
            "perhaps -- tramp\n",
            "ate -- it\n",
            "soap -- some\n",
            "gave -- cave\n",
            "Bullimore -- bullock\n",
            "gave -- cave\n",
            "got -- go\n",
            "beside -- side\n",
            "her -- here\n",
            "her -- here\n",
            "soap -- some\n",
            "what -- was\n",
            "her -- here\n",
            "gave -- cave\n",
            "soap -- some\n",
            "gave -- cave\n",
            "that's -- that\n",
            "killed -- kill\n",
            "wait -- water\n",
            "inject -- neck\n",
            "make -- mack\n",
            "sure -- shore\n",
            "fork -- walk\n",
            "knock -- nock\n",
            "them -- the\n",
            "worm -- worms\n",
            "powder -- wonder\n",
            "worm -- worms\n",
            "get -- ge\n",
            "their -- there\n",
            "them -- theme\n",
            "sure -- shore\n",
            "has -- as\n",
            "can -- came\n",
            "for -- of\n",
            "fattening -- waiting\n",
            "fattening -- waiting\n",
            "when -- the\n",
            "are -- and\n",
            "weighing -- laying\n",
            "weighed -- said\n",
            "killed -- kill\n",
            "too -- to\n",
            "Him -- Im\n",
            "Him -- Im\n",
            "too -- to\n",
            "for -- from\n",
            "they -- the\n",
            "too -- to\n",
            "Troy -- Toy\n",
            "thought -- though\n",
            "Royal -- Royl\n",
            "1036.0 words matched out of 1129.0 words in corrected sentences\n",
            "Accuracy of the current Spell Checker : 0.917626217892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nd7Z7WN3-xWz",
        "colab_type": "code",
        "outputId": "4f6d31e7-f4e8-4a6e-a293-79663c9b61c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'ring' in word_dictionary_lemma"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "HY_L4Ig3-2ct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By updating the spell checker with various improvements, an accuracy of 91.76% is achieved, which is clearly better model than the original spell checker (accuracy of 83.79%)."
      ]
    }
  ]
}